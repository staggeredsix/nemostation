version: "3.9"

services:
  nemotron-vllm:
    image: nvcr.io/nvidia/vllm:25.11-py3
    container_name: nemotron-vllm
    restart: unless-stopped
    entrypoint: ["/bin/bash", "/scripts/vllm_entrypoint.sh"]
    gpus: all
    environment:
      MODEL_ID: ${MODEL_ID:-nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16}
      HF_TOKEN: ${HF_TOKEN:-}
      HF_HOME: ${HF_HOME:-/root/.cache/huggingface}
      RMS_NORM_EPS: ${RMS_NORM_EPS:-}
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: ${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
    ipc: host
    shm_size: "8gb"
    ulimits:
      memlock: -1
      stack: 67108864
    volumes:
      - ./_hf_cache:/root/.cache/huggingface
      - ./scripts:/scripts:ro
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 10s
      timeout: 10s
      retries: 300
      start_period: 30m

  nemotron-ui:
    build:
      context: .
      dockerfile: Dockerfile.ui
    container_name: nemotron-ui
    restart: unless-stopped
    environment:
      OPENAI_BASE_URL: http://nemotron-vllm:8000/v1
      GRADIO_SERVER_NAME: 0.0.0.0
      GRADIO_SERVER_PORT: 7860
    ports:
      - "7860:7860"
    volumes:
      - ./prompt_library:/app/prompt_library
      - ./context:/app/context:ro
      - ./prompts:/app/prompts:ro
    depends_on:
      nemotron-vllm:
        condition: service_healthy
